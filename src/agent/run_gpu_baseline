#!/bin/bash

if [ -z "$1" ]
  then echo "Please provide the name of the game, e.g.  ./run_gpu breakout "; exit 0
fi
ENV=$1

game_path=$PWD"/roms/"
agent="NeuralQLearner"
n_replay=1
ee_n_replay=1
netfile="\"convnet_atari3\""
ee_netfile="\"convnet_atari3_ee\""
update_freq=4
image_compare_update_freq=4
actrep=4
#seed=1
seed=$RANDOM
pool_frms_type="\"max\""
pool_frms_size=2
initial_priority="false"
preproc_net="\"net_downsample_2x_full_y\""
state_dim=7056 #Calculated from downscaled image dimensions of 84 x 84
ncols=1

################
# ALE settings #
################

useRGB=false
terminal_on_life_loss=true
repeat_action_probability=0.25

env_params="useRGB=$useRGB,terminal_on_life_loss=$terminal_on_life_loss,repeat_action_probability=$repeat_action_probability"

################
# Key settings #
################

logfile_dir="\"logs/\""
agent_type="results_baseline_gpu1"
agent_name=$agent_type"_"$1"_FULL_Y"
#saved_net=$PWD"/dqn/testing_montezuma_revenge_FULL_Y_q_net.t7"
#ee_saved_net="/home/mickey/Dropbox/Atari/Logs/ForLoading/results_our_method_montezuma_revenge_FULL_Y_dist_net.t7"
#ee_saved_net="/home/ubuntu/Atari/Logs/ForLoading/results_our_method_montezuma_revenge_FULL_Y_dist_net.t7"
save_freq=25000 #25000, default is 125000
thicken_image="false"
dump_convolutional_filters="false"

##############################
# Shared training parameters #
##############################

replay_memory=1000000 #Reduce this if you're running out of memory

######################################
# Training parameters for Q-Learning #
######################################

q_learn_min_partitions=0
double_dqn="true"
exploration_style="\"eGreedy\"" #eGreedy,softmax
eps_end=0.01 #0.05 From long Unifying paper: In particular, we use the DQN e decay schedule in order to produce reasonable exploration for DQN; additional experiments suggest that using exploration bonuses removes the need for such a schedule.
eps_endt=1000000
non_reward_frames_before_full_eps=500
q_learn_MC_proportion=0.1
max_nodes=100
learn_start=50000 #250000 #50000
discount=0.99
lr=0.00025 # Learning rate for RMSProp

##########################
# Pseudo reward settings #
##########################

pseudocount_rewards_on="false"
ee_beta=0
max_pellet_reward=0

##############################################
# Training parameters for exploration effort #
##############################################

do_time_flip="false"
training_method="\"adam\"" #adam,sgd,rmsprop,rmsprop_dqn

# Adam settings
adam_epsilon=0.00015

# RMS Prop settings
rms_prop_epsilon=0.01

evaluation_mode_required="false" #Set to "true" if using dropout (and care about stats)

# Regularisation
reg_lambda=0 #0.001 #0.01 #0.01
reg_alpha=0

# Other
logger_steps=2000
debug_state_change_rate=500
reward_scale=1 #This is kappa
ee_comparison_policy="\"uniform\"" #uniform,current
ee_time_sep_constant_m=100 #500
ee_discount=0.99
ee_MC_proportion=0.1
ee_MC_clip=0.5
ee_learn_start=999999999 #250000 #The size of the experience cache when learning begins. Don't make larger than the experience cache size!
ee_learn_end=0 #2000000
ee_histLen=1
ee_histSpacing=1
lr_image_compare=0.0000625 #0.1 #0.001 for sgd
lr_end_image_compare=0.0000625 #0.1 #0.001 for sgd
lr_endt_image_compare=250000
mom_image_compare=0.95 # Not applicable to all training methods

#######################
# Node logic settings #
#######################

partition_selection_style="\"closest\"" #oldest,closest
stored_displacements_refresh_steps=999999999 #250
partition_creation_start=999999999 #0 #999999999
partition_update_freq=4
partition_add_initial_time_gap=999999999 #5000
partition_add_time_mult=1.2 #1.25
partition_visit_rate_mom=0.99

agent_params="game_name=\""$ENV"\",lr="$lr",lr_image_compare="$lr_image_compare",lr_end_image_compare="$lr_end_image_compare",lr_endt_image_compare="$lr_endt_image_compare",reg_lambda="$reg_lambda",reg_alpha="$reg_alpha",thicken_image="$thicken_image",dump_convolutional_filters="$dump_convolutional_filters",logfile_dir="$logfile_dir",mom_image_compare="$mom_image_compare",ep=1,ep_end="$eps_end",ep_endt="$eps_endt",discount="$discount",hist_len=4,ee_histLen="$ee_histLen",ee_histSpacing="$ee_histSpacing",q_learn_min_partitions="$q_learn_min_partitions",double_dqn="$double_dqn",exploration_style="$exploration_style",max_nodes="$max_nodes",q_learn_MC_proportion="$q_learn_MC_proportion",non_reward_frames_before_full_eps="$non_reward_frames_before_full_eps",learn_start="$learn_start",ee_learn_start="$ee_learn_start",ee_learn_end="$ee_learn_end",partition_creation_start="$partition_creation_start",partition_update_freq="$partition_update_freq",do_time_flip="$do_time_flip",logger_steps="$logger_steps",debug_state_change_rate="$debug_state_change_rate",partition_add_initial_time_gap="$partition_add_initial_time_gap",partition_add_time_mult="$partition_add_time_mult",partition_visit_rate_mom="$partition_visit_rate_mom",max_pellet_reward="$max_pellet_reward",reward_scale="$reward_scale",ee_comparison_policy="$ee_comparison_policy",ee_time_sep_constant_m="$ee_time_sep_constant_m",ee_discount="$ee_discount",ee_MC_proportion="$ee_MC_proportion",ee_MC_clip="$ee_MC_clip",training_method="$training_method",adam_epsilon="$adam_epsilon",rms_prop_epsilon="$rms_prop_epsilon",evaluation_mode_required="$evaluation_mode_required",partition_selection_style="$partition_selection_style",stored_displacements_refresh_steps="$stored_displacements_refresh_steps",pseudocount_rewards_on="$pseudocount_rewards_on",ee_beta="$ee_beta",replay_memory="$replay_memory",update_freq="$update_freq",image_compare_update_freq="$image_compare_update_freq",n_replay="$n_replay",ee_n_replay="$ee_n_replay",network="$netfile",image_compare_network="$ee_netfile",preproc="$preproc_net",state_dim="$state_dim",minibatch_size=32,rescale_r=1,ncols="$ncols",bufferSize=512,valid_size=500,target_q=10000,clip_delta=1,min_reward=-1,max_reward=1"
steps=50000000
eval_freq=999999999 #default is 250000
eval_steps=1 #default is 125000
prog_freq=10000
gpu=1 #-1 = cpu, 0+ = gpu
random_starts=0 #30 used for ICML
pool_frms="type="$pool_frms_type",size="$pool_frms_size
num_threads=4

args="-game_path $game_path -name $agent_name -env $ENV -env_params $env_params -agent $agent -agent_params $agent_params -steps $steps -eval_freq $eval_freq -eval_steps $eval_steps -prog_freq $prog_freq -save_freq $save_freq -actrep $actrep -gpu $gpu -random_starts $random_starts -pool_frms $pool_frms -seed $seed -threads $num_threads"

#These effectively override the above args
if [ -z ${saved_net+x} ]; then echo "Not loading from file"; else echo "Loading from file $saved_net"; args="$args -network $saved_net"; fi

if [ -z ${ee_saved_net+x} ]; then echo "Not loading image compare net from file"; else echo "Loading image compare net from file $ee_saved_net"; args="$args -image_compare_network $ee_saved_net"; fi

echo $args

make
qlua train_agent.lua $args &
